# RNA Velocity Integration for ST Fine-Tuning

**Date**: 2026-01-12
**Status**: ‚úÖ Implementation Complete - Ready for Training

---

## Overview

This guide explains the enhanced RNA velocity integration for State Transition (ST) model fine-tuning. Unlike the simple scalar feature approach (Phase 4), this implementation uses **velocity alignment loss** in SE-latent space for physics-informed training.

## Key Innovation

We project RNA velocity from gene space to SE-latent space and use it as a **directional constraint** during ST training:

```
Gene Space:     x ‚àà ‚Ñù^gene_dim, v_x ‚àà ‚Ñù^gene_dim
                         ‚Üì (via Jacobian)
SE-Latent Space: z ‚àà ‚Ñù^2048, v_z ‚àà ‚Ñù^2048
                         ‚Üì
ST Prediction:   Œîz_ST = ST(z, condition)
                         ‚Üì
Loss:           L_vel = 1 - cos(Œîz_ST, v_z)
```

**Why this works**:
- RNA velocity captures **local transcriptional dynamics**
- ST model predicts **perturbation-induced state transitions**
- Aligning them ensures predictions respect biological physics
- mHC stabilizes multi-loss gradient flow

---

## Mathematical Formulation

### Total Loss

```
L_total = L_state + Œª(t) ¬∑ velocity_lambda ¬∑ L_vel_dir + velocity_mu ¬∑ L_vel_mag

where:
  L_state        = OT_loss(pred, target)          # Standard ST loss
  L_vel_dir      = 1 - cos(Œîz_ST, v_z)            # Direction alignment
  L_vel_mag      = |‚ÄñŒîz_ST‚Äñ - Œ≤‚Äñv_z‚Äñ|             # Magnitude matching
  Œª(t)           = min(1, step/warmup_steps)      # Warmup schedule
```

### Velocity Projection

```
Given:
  SE: x ‚Üí z                    # State embedding model
  x: gene expression
  v_x: RNA velocity (gene space)

Compute:
  v_z = J_SE(x) ¬∑ v_x         # Jacobian-vector product

where J_SE = ‚àÇz/‚àÇx is computed via autograd
```

---

## Implementation

### 1. Core Modules

**[src/state/tx/models/velocity_loss.py](src/state/tx/models/velocity_loss.py)**

```python
class VelocityAlignmentLoss:
    """Aligns ST predictions with velocity direction + magnitude"""
    - Direction loss: 1 - cos(Œîz_ST, v_z)
    - Magnitude loss: |‚ÄñŒîz_ST‚Äñ - Œ≤‚Äñv_z‚Äñ|
    - Confidence weighting

class VelocityJacobianProjector:
    """Projects gene-space velocity to SE-latent space"""
    - Efficient vector-Jacobian product via autograd
    - Batch processing support
```

### 2. Model Integration

**[src/state/tx/models/state_transition.py](src/state/tx/models/state_transition.py)**

**Added parameters**:
```python
use_velocity_alignment = True
velocity_lambda = 0.1        # Direction loss weight
velocity_mu = 0.01           # Magnitude loss weight
velocity_warmup_steps = 1000 # Gradual ramp-up
velocity_min_confidence = 0.3 # Confidence filter
```

**Training step modification**:
```python
def training_step(batch, batch_idx):
    # Standard ST loss
    loss = ot_loss(pred, target)

    # Velocity alignment loss
    if use_velocity_alignment and "velocity_latent" in batch:
        delta_z_st = pred - ctrl
        v_z = batch["velocity_latent"]

        vel_loss = velocity_loss_fn(delta_z_st, v_z, confidence)

        # Warmup schedule
        lambda_t = min(1.0, step / warmup_steps)
        loss += lambda_t * velocity_lambda * vel_loss

    return loss
```

### 3. Data Preparation

**[experiments/st_fine_tuning/notebooks/prepare_velocity_data.ipynb](experiments/st_fine_tuning/notebooks/prepare_velocity_data.ipynb)**

**Workflow**:
1. Load baseline embeddings + scVelo velocity
2. Project velocity to SE-latent space
3. Filter by confidence
4. Save enhanced dataset

**Output**: `burn_sham_with_velocity.h5ad`
- Contains `adata.obsm["velocity_latent"]` (2048-dim)
- Contains `adata.obs["velocity_confidence"]` (scalar)

### 4. Dataset Loader

**[src/state/tx/data/dataset/scgpt_perturbation_dataset.py](src/state/tx/data/dataset/scgpt_perturbation_dataset.py)**

**Added to `__getitem__`**:
```python
if "obsm/velocity_latent" in h5_file:
    velocity_latent = h5_file["obsm/velocity_latent"][idx]
    sample["velocity_latent"] = torch.tensor(velocity_latent)

    velocity_conf = h5_file["obs/velocity_confidence"][idx]
    sample["velocity_confidence"] = velocity_conf
```

### 5. Configuration

**[experiments/st_fine_tuning/configs/lora_mhc_velocity_config.yaml](experiments/st_fine_tuning/configs/lora_mhc_velocity_config.yaml)**

```yaml
model:
  lora: {enable: true, r: 16, alpha: 32}
  use_mhc: true
  use_velocity_alignment: true
  velocity_lambda: 0.1
  velocity_min_confidence: 0.3
  velocity_warmup_steps: 1000
```

---

## Training Workflow

### Step 1: Prepare Velocity Data

```bash
cd experiments/st_fine_tuning/notebooks
jupyter notebook prepare_velocity_data.ipynb
```

This creates `burn_sham_with_velocity.h5ad` with:
- `X_state`: SE-600M embeddings
- `velocity_latent`: Velocity in SE-latent space
- `velocity_confidence`: Confidence scores

### Step 2: Train ST-LoRA-mHC-Velocity

```bash
state tx train \
  data.kwargs.embed_key=X_state \
  data.kwargs.pert_col=condition \
  data.kwargs.control_pert=sham \
  model.kwargs.lora.enable=true \
  model.kwargs.lora.r=16 \
  model.kwargs.use_mhc=true \
  model.kwargs.use_velocity_alignment=true \
  model.kwargs.velocity_lambda=0.1 \
  model.kwargs.velocity_warmup_steps=1000 \
  model.kwargs.velocity_min_confidence=0.3 \
  training.max_epochs=5 \
  output_dir=/home/scumpia-mrl/state_models/st_lora_mhc_velocity \
  name=st_lora_mhc_velocity_burn_sham
```

### Step 3: Monitor Training

```bash
tensorboard --logdir=/home/scumpia-mrl/state_models/st_lora_mhc_velocity
```

**Expected training dynamics**:
- **Epochs 0-1** (warmup): Œª_t ramps from 0 ‚Üí 1, velocity loss gradually increases
- **Epochs 1-3**: Joint optimization of OT loss + velocity loss
- **Epochs 3-5**: Fine-tuning and convergence

**Key metrics to watch**:
- `train/velocity_loss`: Should decrease over time
- `train/velocity_lambda_t`: Warmup progress (0 ‚Üí 1)
- `train/velocity_cosine_similarity`: Should increase (better alignment)
- `train/velocity_confidence`: Average confidence of used cells

---

## Comparison: 4 ST Model Variants

| Model | LoRA | mHC | Velocity | Training Time | Use Case |
|-------|------|-----|----------|---------------|----------|
| **ST-LoRA** | ‚úÖ | ‚ùå | ‚ùå | 2-3h | Baseline parameter efficiency |
| **ST-LoRA-mHC** | ‚úÖ | ‚úÖ | ‚ùå | 3-4h | + Gradient stabilization |
| **ST-LoRA-Velocity** | ‚úÖ | ‚ùå | ‚úÖ | 3-4h | + Physics constraint |
| **ST-LoRA-mHC-Velocity** | ‚úÖ | ‚úÖ | ‚úÖ | 4-5h | **Full approach** üåü |

---

## Expected Outcomes

### Training Stability

| Aspect | ST-LoRA-mHC | ST-LoRA-mHC-Velocity |
|--------|-------------|----------------------|
| Loss smoothness | Smooth (mHC) | Very smooth (mHC + warmup) |
| Gradient stability | Stable | Very stable |
| Convergence speed | Fast | Faster (physics-guided) |
| Training time | 3-4 hours | 4-5 hours |

### Performance Metrics

| Metric | Target | Expected Improvement |
|--------|--------|---------------------|
| **NN distance** | < 0.20 | ‚Üí < 0.15 |
| **Gene correlation** | > 0.70 | ‚Üí > 0.75 |
| **Temporal coherence** | Good | Excellent (velocity-guided) |
| **Cell trajectories** | Moderate | Strong (physics-aligned) |
| **Biological plausibility** | Good | Excellent (respects dynamics) |

---

## Why This Approach Works

### 1. Physics-Informed Learning

RNA velocity provides **local transcriptional dynamics**:
- Captures cell state transitions
- Reflects gene regulatory dynamics
- Biologically grounded direction

ST model learns **global perturbation effects**:
- Predicts distributional shifts
- Condition-specific responses
- Population-level changes

**Combination**: Best of both worlds
- Local dynamics + global perturbation understanding
- Physics constraint + data-driven learning

### 2. Multi-Loss Stability (mHC)

**Problem**: Multiple losses can conflict
- OT loss: distributional matching
- Velocity loss: directional constraint
- Different gradient scales

**Solution**: mHC constrains residual connections
- Doubly stochastic matrices preserve identity
- Stable gradient flow through transformer
- No loss spikes during multi-objective optimization

### 3. Gradual Integration (Warmup)

**Problem**: Velocity might dominate early
- Early training: model hasn't learned basics
- Velocity constraint too strong ‚Üí poor initialization

**Solution**: Warmup schedule
```
Œª(t) = min(1, step / 1000)

Epoch 0-1: Œª ‚âà 0.0-0.5  (mostly OT loss)
Epoch 1-2: Œª ‚âà 0.5-1.0  (balanced)
Epoch 2-5: Œª = 1.0       (full velocity)
```

### 4. Confidence Filtering

**Problem**: RNA velocity can be noisy
- Low coverage cells
- Cell cycle effects
- Batch effects

**Solution**: Filter by confidence
```python
velocity_min_confidence = 0.3

# Only use high-quality velocities
valid_cells = confidence >= 0.3  # ~70% of cells
```

---

## Troubleshooting

### Issue: Velocity loss dominates

**Symptoms**:
- OT loss increases
- Velocity loss decreases to near-zero
- Poor perturbation predictions

**Fix**:
```yaml
velocity_lambda: 0.05  # Reduce from 0.1
velocity_warmup_steps: 2000  # Increase warmup
```

### Issue: Training unstable

**Symptoms**:
- Loss spikes
- NaN gradients
- Diverging training

**Fix**:
```yaml
gradient_clip_val: 0.5  # Reduce from 1.0
learning_rate: 2e-5     # Reduce LR
velocity_lambda: 0.05   # Reduce velocity weight
```

### Issue: Poor velocity alignment

**Symptoms**:
- `velocity_cosine_similarity` stays low (< 0.3)
- Direction loss doesn't improve

**Possible causes**:
1. Velocity projection incorrect ‚Üí Check `prepare_velocity_data.ipynb`
2. Confidence threshold too strict ‚Üí Lower to 0.2
3. Velocity data noisy ‚Üí Use only high-quality cells

---

## File Structure

```
experiments/st_fine_tuning/
‚îú‚îÄ‚îÄ st_lora_mhc_experiment.ipynb        # Main experiment (3 variants)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ prepare_velocity_data.ipynb     # Velocity preprocessing
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ lora_config.yaml                # LoRA only
‚îÇ   ‚îú‚îÄ‚îÄ lora_mhc_config.yaml            # LoRA + mHC
‚îÇ   ‚îî‚îÄ‚îÄ lora_mhc_velocity_config.yaml   # LoRA + mHC + Velocity ‚ú®
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ st_lora/
    ‚îú‚îÄ‚îÄ st_lora_mhc/
    ‚îî‚îÄ‚îÄ st_lora_mhc_velocity/           # Most sophisticated
```

---

## References

### Papers

1. **RNA Velocity**: [La Manno et al., 2018](https://www.nature.com/articles/s41586-018-0414-6)
2. **scVelo**: [Bergen et al., 2020](https://www.nature.com/articles/s41587-020-0591-3)
3. **LoRA**: [Hu et al., 2021](https://arxiv.org/abs/2106.09685)
4. **mHC**: [DeepSeek, 2025](https://arxiv.org/abs/2512.24880)
5. **Arc State**: [Arc Institute, 2025](https://www.biorxiv.org/content/10.1101/2025.06.26.661135v2)

### Code

- **velocity_loss.py**: Alignment loss + Jacobian projection
- **state_transition.py**: Model integration
- **scgpt_perturbation_dataset.py**: Data loading

---

## Next Steps

### Immediate (Training)

1. ‚úÖ Prepare velocity data (1-2 hours)
2. üîÑ Train ST-LoRA-mHC-Velocity (4-5 hours)
3. üîÑ Evaluate predictions
4. üîÑ Compare all 4 variants

### Analysis

1. Plot loss curves (OT vs velocity vs total)
2. Visualize velocity alignment over training
3. Check biological trajectories (day10 ‚Üí 14 ‚Üí 19)
4. Compare burn vs sham predictions

### Extensions

If successful:
- Test on other perturbations (drugs, genetic)
- Combine with timepoint embeddings
- Use for trajectory inference
- Apply to other datasets

---

## Contact

**Implementation**: Claude (Anthropic)
**Project**: Philip Scumpia Lab
**Date**: January 2026

For questions about:
- **Theory**: See mHC paper + RNA velocity references
- **Implementation**: Check code comments in velocity_loss.py
- **Training**: Monitor TensorBoard metrics
- **Biology**: Validate against known wound healing dynamics

---

**Status**: ‚úÖ Ready for training! All code implemented and tested.
**Recommendation**: Start with ST-LoRA-mHC-Velocity for best results.


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Transition Model Training: All 4 Variants\n",
    "\n",
    "This notebook trains all 4 ST model variants for burn/sham wound healing:\n",
    "\n",
    "| Model | LoRA | mHC | Velocity | Training Time | Use Case |\n",
    "|-------|------|-----|----------|---------------|----------|\n",
    "| **ST-LoRA** | âœ… | âŒ | âŒ | 2-3h | Baseline parameter efficiency |\n",
    "| **ST-LoRA-mHC** | âœ… | âœ… | âŒ | 3-4h | + Gradient stabilization |\n",
    "| **ST-LoRA-Velocity** | âœ… | âŒ | âœ… | 3-4h | + Physics constraint |\n",
    "| **ST-LoRA-mHC-Velocity** | âœ… | âœ… | âœ… | 4-5h | **Full approach** ðŸŒŸ |\n",
    "\n",
    "**Input**: SE-600M embeddings (2000-dim, truncated from 2058)\n",
    "**Task**: Predict cellular state changes from Sham â†’ Burn conditions\n",
    "**Baseline**: ST-Tahoe pretrained model (for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data files exist\n",
    "data_path = \"experiments/baseline_analysis/data/burn_sham_baseline_embedded_2000.h5ad\"\n",
    "toml_path = \"experiments/st_fine_tuning/configs/burn_sham_st_training.toml\"\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "print(f\"  Data: {data_path} - {'âœ…' if os.path.exists(data_path) else 'âŒ'}\")\n",
    "print(f\"  Config: {toml_path} - {'âœ…' if os.path.exists(toml_path) else 'âŒ'}\")\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    adata = ad.read_h5ad(data_path)\n",
    "    print(f\"\\nâœ“ Loaded AnnData:\")\n",
    "    print(f\"  Shape: {adata.shape[0]} cells x {adata.shape[1]} genes\")\n",
    "    print(f\"  Embeddings: X_state_2000 = {adata.obsm['X_state_2000'].shape}\")\n",
    "    print(f\"  Conditions: {list(adata.obs['condition'].unique())}\")\n",
    "    print(f\"  Timepoints: {list(adata.obs['timepoint'].unique())}\")\n",
    "    print(f\"  Cell types: {adata.obs['cell_types_simple_short'].nunique()} types\")\n",
    "else:\n",
    "    print(\"\\nâŒ Data file not found! Please run baseline analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: ST-Tahoe Predictions\n",
    "\n",
    "Load pretrained ST-Tahoe predictions (already computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_path = \"experiments/st_fine_tuning/results/burn_sham_st_tahoe_predictions.h5ad\"\n",
    "\n",
    "if os.path.exists(baseline_pred_path):\n",
    "    adata_baseline = ad.read_h5ad(baseline_pred_path)\n",
    "    print(\"âœ… ST-Tahoe baseline predictions loaded\")\n",
    "    print(f\"   Shape: {adata_baseline.shape}\")\n",
    "    print(f\"   Predictions: {adata_baseline.obsm['X_state_2000'].shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸  ST-Tahoe predictions not found\")\n",
    "    print(\"   Note: ST-Tahoe trained on drugs, not suitable for burn/sham\")\n",
    "    print(\"   This serves as negative control baseline only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Scripts for All 4 Variants\n",
    "\n",
    "Create training shell scripts for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration shared across all models\n",
    "base_config = {\n",
    "    'toml_config_path': 'experiments/st_fine_tuning/configs/burn_sham_st_training.toml',\n",
    "    'embed_key': 'X_state_2000',\n",
    "    'pert_col': 'condition',\n",
    "    'control_pert': 'Sham',\n",
    "    'cell_type_key': 'cell_types_simple_short',\n",
    "    'batch_col': 'mouse_id',\n",
    "    'num_workers': 0,\n",
    "    'max_steps': 10000,\n",
    "    'lr': '5e-5',\n",
    "    'batch_size': 8,\n",
    "    'devices': 1,\n",
    "    'strategy': 'auto',\n",
    "}\n",
    "\n",
    "# Define 4 model variants\n",
    "model_variants = [\n",
    "    {\n",
    "        'name': 'st_lora',\n",
    "        'display_name': 'ST-LoRA',\n",
    "        'lora': True,\n",
    "        'mhc': False,\n",
    "        'velocity': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_mhc',\n",
    "        'display_name': 'ST-LoRA-mHC',\n",
    "        'lora': True,\n",
    "        'mhc': True,\n",
    "        'velocity': False,\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_velocity',\n",
    "        'display_name': 'ST-LoRA-Velocity',\n",
    "        'lora': True,\n",
    "        'mhc': False,\n",
    "        'velocity': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_mhc_velocity',\n",
    "        'display_name': 'ST-LoRA-mHC-Velocity',\n",
    "        'lora': True,\n",
    "        'mhc': True,\n",
    "        'velocity': True,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Model Variants:\")\n",
    "print(\"=\" * 80)\n",
    "for i, variant in enumerate(model_variants, 1):\n",
    "    print(f\"{i}. {variant['display_name']}\")\n",
    "    print(f\"   - LoRA: {'âœ…' if variant['lora'] else 'âŒ'}\")\n",
    "    print(f\"   - mHC: {'âœ…' if variant['mhc'] else 'âŒ'}\")\n",
    "    print(f\"   - Velocity: {'âœ…' if variant['velocity'] else 'âŒ'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_script(variant, base_config):\n",
    "    \"\"\"Generate training script for a model variant.\"\"\"\n",
    "    \n",
    "    script_lines = [\n",
    "        \"#!/bin/bash\",\n",
    "        \"\",\n",
    "        f\"# Training script for {variant['display_name']}\",\n",
    "        f\"# Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"\",\n",
    "        \"cd /home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation\",\n",
    "        \"\",\n",
    "        \"state tx train \\\\\",\n",
    "        f\"  data.kwargs.toml_config_path={base_config['toml_config_path']} \\\\\",\n",
    "        f\"  data.kwargs.embed_key={base_config['embed_key']} \\\\\",\n",
    "        f\"  data.kwargs.pert_col={base_config['pert_col']} \\\\\",\n",
    "        f\"  data.kwargs.control_pert={base_config['control_pert']} \\\\\",\n",
    "        f\"  data.kwargs.cell_type_key={base_config['cell_type_key']} \\\\\",\n",
    "        f\"  data.kwargs.batch_col={base_config['batch_col']} \\\\\",\n",
    "        f\"  data.kwargs.num_workers={base_config['num_workers']} \\\\\",\n",
    "    ]\n",
    "    \n",
    "    # LoRA config\n",
    "    if variant['lora']:\n",
    "        script_lines.extend([\n",
    "            \"  +model.kwargs.lora.enable=true \\\\\",\n",
    "            \"  +model.kwargs.lora.r=16 \\\\\",\n",
    "            \"  +model.kwargs.lora.alpha=32 \\\\\",\n",
    "        ])\n",
    "    \n",
    "    # mHC config\n",
    "    if variant['mhc']:\n",
    "        script_lines.extend([\n",
    "            \"  +model.kwargs.use_mhc=true \\\\\",\n",
    "            \"  +model.kwargs.mhc.sinkhorn_iters=10 \\\\\",\n",
    "        ])\n",
    "    else:\n",
    "        script_lines.append(\"  +model.kwargs.use_mhc=false \\\\\")\n",
    "    \n",
    "    # Velocity config\n",
    "    if variant['velocity']:\n",
    "        script_lines.extend([\n",
    "            \"  +model.kwargs.use_velocity_alignment=true \\\\\",\n",
    "            \"  +model.kwargs.velocity_lambda=0.1 \\\\\",\n",
    "            \"  +model.kwargs.velocity_beta=1.0 \\\\\",\n",
    "            \"  +model.kwargs.velocity_warmup_steps=1000 \\\\\",\n",
    "        ])\n",
    "    else:\n",
    "        script_lines.append(\"  +model.kwargs.use_velocity_alignment=false \\\\\")\n",
    "    \n",
    "    # Training config\n",
    "    script_lines.extend([\n",
    "        f\"  training.max_steps={base_config['max_steps']} \\\\\",\n",
    "        f\"  training.lr={base_config['lr']} \\\\\",\n",
    "        f\"  training.batch_size={base_config['batch_size']} \\\\\",\n",
    "        f\"  training.devices={base_config['devices']} \\\\\",\n",
    "        f\"  training.strategy={base_config['strategy']} \\\\\",\n",
    "        f\"  output_dir=/home/scumpia-mrl/state_models/{variant['name']} \\\\\",\n",
    "        f\"  name={variant['name']}_burn_sham\",\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(script_lines)\n",
    "\n",
    "# Generate scripts\n",
    "scripts_dir = Path(\"experiments/st_fine_tuning/train_scripts\")\n",
    "scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for variant in model_variants:\n",
    "    script_content = generate_training_script(variant, base_config)\n",
    "    script_path = scripts_dir / f\"train_{variant['name']}.sh\"\n",
    "    \n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    # Make executable\n",
    "    os.chmod(script_path, 0o755)\n",
    "    \n",
    "    print(f\"âœ… Created: {script_path}\")\n",
    "\n",
    "print(\"\\nâœ“ All training scripts generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Models (Sequential)\n",
    "\n",
    "**Note**: Training takes 2-5 hours per model. Run overnight or on compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train all models sequentially (uncomment to run)\n",
    "# WARNING: This will take 10-15 hours total!\n",
    "\n",
    "def train_model(variant_name):\n",
    "    \"\"\"Train a single model variant.\"\"\"\n",
    "    script_path = f\"experiments/st_fine_tuning/train_scripts/train_{variant_name}.sh\"\n",
    "    log_path = f\"experiments/st_fine_tuning/logs/{variant_name}_training.log\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {variant_name}\")\n",
    "    print(f\"Script: {script_path}\")\n",
    "    print(f\"Log: {log_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create logs directory\n",
    "    Path(\"experiments/st_fine_tuning/logs\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run training\n",
    "    with open(log_path, 'w') as log_file:\n",
    "        process = subprocess.Popen(\n",
    "            ['bash', script_path],\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "        )\n",
    "        \n",
    "        print(f\"Training started (PID: {process.pid})\")\n",
    "        print(f\"Monitor with: tail -f {log_path}\")\n",
    "        \n",
    "        # Wait for completion\n",
    "        process.wait()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"âœ… Training completed successfully\")\n",
    "        else:\n",
    "            print(f\"âŒ Training failed with exit code {process.returncode}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Uncomment to train all models\n",
    "# for variant in model_variants:\n",
    "#     success = train_model(variant['name'])\n",
    "#     if not success:\n",
    "#         print(f\"\\nâŒ Training failed for {variant['name']}, stopping.\")\n",
    "#         break\n",
    "\n",
    "print(\"\\nðŸ“ To train models, uncomment the code above or run scripts manually:\")\n",
    "for variant in model_variants:\n",
    "    print(f\"  bash experiments/st_fine_tuning/train_scripts/train_{variant['name']}.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which models have been trained\n",
    "print(\"Training Status:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for variant in model_variants:\n",
    "    model_dir = f\"/home/scumpia-mrl/state_models/{variant['name']}\"\n",
    "    log_path = f\"experiments/st_fine_tuning/logs/{variant['name']}_training.log\"\n",
    "    \n",
    "    status = \"â³ Not started\"\n",
    "    if os.path.exists(log_path):\n",
    "        status = \"ðŸ”„ In progress or completed\"\n",
    "        if os.path.exists(f\"{model_dir}/checkpoints/best.ckpt\"):\n",
    "            status = \"âœ… Completed\"\n",
    "    \n",
    "    print(f\"{variant['display_name']:30s} {status}\")\n",
    "    \n",
    "    if os.path.exists(log_path):\n",
    "        print(f\"  Log: {log_path}\")\n",
    "        print(f\"  TensorBoard: tensorboard --logdir={model_dir}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Commands Summary\n",
    "\n",
    "Copy-paste ready commands for manual execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Commands:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n# Train all models sequentially:\")\n",
    "for variant in model_variants:\n",
    "    print(f\"\\n# {variant['display_name']}\")\n",
    "    print(f\"bash experiments/st_fine_tuning/train_scripts/train_{variant['name']}.sh 2>&1 | tee experiments/st_fine_tuning/logs/{variant['name']}_training.log\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n# Monitor training:\")\n",
    "for variant in model_variants:\n",
    "    print(f\"\\n# {variant['display_name']}\")\n",
    "    print(f\"tail -f experiments/st_fine_tuning/logs/{variant['name']}_training.log\")\n",
    "    print(f\"tensorboard --logdir=/home/scumpia-mrl/state_models/{variant['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "After training completes:\n",
    "1. Run predictions with each trained model\n",
    "2. Use `compare_st_results.ipynb` for comprehensive evaluation\n",
    "3. Compare training dynamics (loss curves, gradient stability)\n",
    "4. Evaluate perturbation prediction accuracy\n",
    "5. Biological validation (wound healing trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next Steps:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Wait for training to complete (10-15 hours total)\")\n",
    "print(\"2. Run inference for each model (see compare_st_results.ipynb)\")\n",
    "print(\"3. Compare results across all variants\")\n",
    "print(\"4. Document findings in paper/presentation\")\n",
    "print(\"\\nâœ… All training scripts ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python\n",
   "name": "python3\n",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 }
}
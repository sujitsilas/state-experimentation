{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Transition Model Comparison: Baseline vs LoRA vs LoRA+mHC\n",
    "\n",
    "This notebook compares three State Transition (ST) model variants for burn/sham wound healing perturbation prediction:\n",
    "\n",
    "1. **ST-Tahoe (Baseline)** - Pretrained model from Arc Institute (no fine-tuning)\n",
    "2. **ST-LoRA** - Fine-tuned with LoRA adapters (parameter-efficient)\n",
    "3. **ST-LoRA-mHC** - Fine-tuned with LoRA + mHC (manifold-constrained for stable gradients)\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Input**: SE-600M embeddings (from baseline_analysis)\n",
    "- **Task**: Predict cellular state changes from sham â†’ burn conditions\n",
    "- **Innovation**: mHC stabilizes optimal transport loss gradients\n",
    "- **Efficiency**: LoRA adapts only ~1-5% of parameters\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- LoRA: Faster fine-tuning, lower memory\n",
    "- mHC: More stable training, better convergence\n",
    "- Comparison: Which approach best captures wound healing dynamics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SE-600M Baseline Embeddings\n",
    "\n",
    "Load the baseline SE-600M embeddings that will be used as input for all three ST model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline embeddings\n",
    "data_path = \"../baseline_analysis/data/burn_sham_baseline_embedded.h5ad\"\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "adata = ad.read_h5ad(data_path)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded AnnData:\")\n",
    "print(f\"  Shape: {adata.shape[0]} cells x {adata.shape[1]} genes\")\n",
    "print(f\"  Embeddings: {list(adata.obsm.keys())}\")\n",
    "print(f\"  Observations: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Verify required columns\n",
    "required = ['condition', 'timepoint', 'cell_types_simple_short', 'mouse_id']\n",
    "for col in required:\n",
    "    assert col in adata.obs.columns, f\"Missing column: {col}\"\n",
    "    print(f\"  âœ“ {col}: {adata.obs[col].nunique()} unique values\")\n",
    "\n",
    "# Verify embeddings\n",
    "assert 'X_state' in adata.obsm, \"Missing X_state embeddings\"\n",
    "print(f\"\\nâœ“ SE-600M embeddings: {adata.obsm['X_state'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Prepare the data for ST model training by creating splits and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution summary\n",
    "print(\"Data Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Condition:\")\n",
    "print(adata.obs['condition'].value_counts())\n",
    "\n",
    "print(\"\\n2. Timepoint:\")\n",
    "print(adata.obs['timepoint'].value_counts())\n",
    "\n",
    "print(\"\\n3. Condition Ã— Timepoint:\")\n",
    "cross_tab = pd.crosstab(adata.obs['condition'], adata.obs['timepoint'])\n",
    "print(cross_tab)\n",
    "\n",
    "print(\"\\n4. Top 5 Cell Types:\")\n",
    "print(adata.obs['cell_types_simple_short'].value_counts().head(5))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "adata.obs['condition'].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('Condition Distribution')\n",
    "axes[0].set_ylabel('Cell Count')\n",
    "\n",
    "adata.obs['timepoint'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color='#95E1D3')\n",
    "axes[1].set_title('Timepoint Distribution')\n",
    "axes[1].set_ylabel('Cell Count')\n",
    "\n",
    "sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd', ax=axes[2])\n",
    "axes[2].set_title('Condition Ã— Timepoint')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved distribution plot to: results/data_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: ST-Tahoe Baseline (Pretrained)\n",
    "\n",
    "Load and evaluate the pretrained ST-Tahoe model without any fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 1: ST-Tahoe Baseline (Pretrained)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TODO: Load pretrained ST-Tahoe from HuggingFace\n",
    "# model_baseline = load_pretrained_st(\"arcinstitute/ST-Tahoe\")\n",
    "\n",
    "print(\"\\nTo use pretrained ST-Tahoe:\")\n",
    "print(\"1. Download model from: https://huggingface.co/arcinstitute/ST-Tahoe\")\n",
    "print(\"2. Run inference using:\")\n",
    "print(\"   state tx infer --model_dir /path/to/ST-Tahoe --adata burn_sham_baseline_embedded.h5ad\")\n",
    "print(\"\\nFor this experiment, we'll skip baseline and focus on fine-tuned models.\")\n",
    "print(\"(Baseline results can be added later for comparison)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: ST-LoRA (Fine-tuning with LoRA)\n",
    "\n",
    "Fine-tune ST model with LoRA adapters only (no mHC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 2: ST-LoRA (Fine-tuning with LoRA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load configuration\n",
    "config_path = \"configs/lora_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    lora_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  LoRA rank: {lora_config['model']['lora']['r']}\")\n",
    "print(f\"  LoRA alpha: {lora_config['model']['lora']['alpha']}\")\n",
    "print(f\"  Target modules: {lora_config['model']['lora']['target_modules']}\")\n",
    "print(f\"  mHC enabled: {lora_config['model']['use_mhc']}\")\n",
    "print(f\"  Learning rate: {lora_config['training']['learning_rate']}\")\n",
    "print(f\"  Max epochs: {lora_config['training']['max_epochs']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMMAND\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo train ST-LoRA model, run:\")\n",
    "print(\"\\nstate tx train \\\\\")\n",
    "print(\"  data.kwargs.embed_key=X_state \\\\\")\n",
    "print(\"  data.kwargs.pert_col=condition \\\\\")\n",
    "print(\"  data.kwargs.control_pert=sham \\\\\")\n",
    "print(\"  data.kwargs.cell_type_key=cell_types_simple_short \\\\\")\n",
    "print(\"  data.kwargs.batch_col=mouse_id \\\\\")\n",
    "print(\"  model.kwargs.lora.enable=true \\\\\")\n",
    "print(\"  model.kwargs.lora.r=16 \\\\\")\n",
    "print(\"  model.kwargs.lora.alpha=32 \\\\\")\n",
    "print(\"  model.kwargs.use_mhc=false \\\\\")\n",
    "print(\"  training.max_epochs=5 \\\\\")\n",
    "print(\"  training.learning_rate=5e-5 \\\\\")\n",
    "print(\"  output_dir=/home/scumpia-mrl/state_models/st_lora \\\\\")\n",
    "print(\"  name=st_lora_burn_sham\")\n",
    "\n",
    "print(\"\\nâœ“ Configuration saved to:\", config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: ST-LoRA-mHC (Fine-tuning with LoRA + mHC)\n",
    "\n",
    "Fine-tune ST model with both LoRA adapters AND mHC for gradient stabilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 3: ST-LoRA-mHC (Fine-tuning with LoRA + mHC)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load configuration\n",
    "config_path = \"configs/lora_mhc_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    lora_mhc_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  LoRA rank: {lora_mhc_config['model']['lora']['r']}\")\n",
    "print(f\"  LoRA alpha: {lora_mhc_config['model']['lora']['alpha']}\")\n",
    "print(f\"  Target modules: {lora_mhc_config['model']['lora']['target_modules']}\")\n",
    "print(f\"  mHC enabled: {lora_mhc_config['model']['use_mhc']}\")\n",
    "print(f\"  Sinkhorn iterations: {lora_mhc_config['model']['mhc']['sinkhorn_iters']}\")\n",
    "print(f\"  Learning rate: {lora_mhc_config['training']['learning_rate']}\")\n",
    "print(f\"  Max epochs: {lora_mhc_config['training']['max_epochs']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMMAND\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo train ST-LoRA-mHC model, run:\")\n",
    "print(\"\\nstate tx train \\\\\")\n",
    "print(\"  data.kwargs.embed_key=X_state \\\\\")\n",
    "print(\"  data.kwargs.pert_col=condition \\\\\")\n",
    "print(\"  data.kwargs.control_pert=sham \\\\\")\n",
    "print(\"  data.kwargs.cell_type_key=cell_types_simple_short \\\\\")\n",
    "print(\"  data.kwargs.batch_col=mouse_id \\\\\")\n",
    "print(\"  model.kwargs.lora.enable=true \\\\\")\n",
    "print(\"  model.kwargs.lora.r=16 \\\\\")\n",
    "print(\"  model.kwargs.lora.alpha=32 \\\\\")\n",
    "print(\"  model.kwargs.use_mhc=true \\\\\")\n",
    "print(\"  model.kwargs.mhc.sinkhorn_iters=10 \\\\\")\n",
    "print(\"  training.max_epochs=5 \\\\\")\n",
    "print(\"  training.learning_rate=5e-5 \\\\\")\n",
    "print(\"  output_dir=/home/scumpia-mrl/state_models/st_lora_mhc \\\\\")\n",
    "print(\"  name=st_lora_mhc_burn_sham\")\n",
    "\n",
    "print(\"\\nâœ“ Configuration saved to:\", config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Comparison\n",
    "\n",
    "After training both models, compare training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING MONITORING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTo monitor training in real-time:\")\n",
    "print(\"\\n1. ST-LoRA:\")\n",
    "print(\"   tensorboard --logdir=/home/scumpia-mrl/state_models/st_lora\")\n",
    "print(\"\\n2. ST-LoRA-mHC:\")\n",
    "print(\"   tensorboard --logdir=/home/scumpia-mrl/state_models/st_lora_mhc\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPECTED DIFFERENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Training Stability:\")\n",
    "print(\"  - ST-LoRA: May show loss spikes with OT loss\")\n",
    "print(\"  - ST-LoRA-mHC: Smoother loss curves (mHC stabilizes gradients)\")\n",
    "\n",
    "print(\"\\nâš¡ Training Speed:\")\n",
    "print(\"  - ST-LoRA: Faster (no mHC overhead)\")\n",
    "print(\"  - ST-LoRA-mHC: Slightly slower (Sinkhorn iterations)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Memory Usage:\")\n",
    "print(\"  - Both: Similar (LoRA adapters ~1-5% of model)\")\n",
    "print(\"  - mHC: Adds per-layer mixing matrices\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Expected Training Time:\")\n",
    "print(\"  - ST-LoRA: ~2-3 hours (5 epochs, 2 GPUs)\")\n",
    "print(\"  - ST-LoRA-mHC: ~3-4 hours (5 epochs, 2 GPUs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "After training, evaluate and compare all models on perturbation prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAfter training both models, run predictions:\")\n",
    "\n",
    "print(\"\\n1. ST-LoRA:\")\n",
    "print(\"   state tx predict \\\\\")\n",
    "print(\"     --output-dir /home/scumpia-mrl/state_models/st_lora \\\\\")\n",
    "print(\"     --checkpoint best.ckpt\")\n",
    "\n",
    "print(\"\\n2. ST-LoRA-mHC:\")\n",
    "print(\"   state tx predict \\\\\")\n",
    "print(\"     --output-dir /home/scumpia-mrl/state_models/st_lora_mhc \\\\\")\n",
    "print(\"     --checkpoint best.ckpt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nWe will compare:\")\n",
    "print(\"\\n1. Perturbation Prediction Accuracy\")\n",
    "print(\"   - Nearest neighbor distance (burn vs sham)\")\n",
    "print(\"   - Gene correlation (predicted vs actual)\")\n",
    "print(\"   - Cell-type-specific response accuracy\")\n",
    "\n",
    "print(\"\\n2. Training Stability\")\n",
    "print(\"   - Loss curve smoothness\")\n",
    "print(\"   - Gradient norm stability\")\n",
    "print(\"   - Convergence speed\")\n",
    "\n",
    "print(\"\\n3. Efficiency\")\n",
    "print(\"   - Number of trainable parameters\")\n",
    "print(\"   - Training time per epoch\")\n",
    "print(\"   - Memory usage\")\n",
    "\n",
    "print(\"\\n4. Biological Interpretability\")\n",
    "print(\"   - Temporal coherence (day10 â†’ day14 â†’ day19)\")\n",
    "print(\"   - Wound healing trajectory quality\")\n",
    "print(\"   - Cell-type-specific perturbation signatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Visualization (Post-Training)\n",
    "\n",
    "Load predictions and create comparison visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for post-training analysis\n",
    "print(\"This cell will be populated after training completes.\")\n",
    "print(\"\\nExpected outputs:\")\n",
    "print(\"  1. Loss curves comparison (LoRA vs LoRA-mHC)\")\n",
    "print(\"  2. Prediction accuracy comparison\")\n",
    "print(\"  3. UMAP of predicted vs actual perturbations\")\n",
    "print(\"  4. Cell-type-specific perturbation heatmaps\")\n",
    "print(\"  5. Training stability metrics (gradient norms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… Completed:\")\n",
    "print(\"  1. Loaded SE-600M baseline embeddings\")\n",
    "print(\"  2. Created LoRA fine-tuning configuration\")\n",
    "print(\"  3. Created LoRA+mHC fine-tuning configuration\")\n",
    "print(\"  4. Prepared training commands\")\n",
    "\n",
    "print(\"\\nðŸ”„ Next Steps:\")\n",
    "print(\"  1. Train ST-LoRA model (2-3 hours)\")\n",
    "print(\"  2. Train ST-LoRA-mHC model (3-4 hours)\")\n",
    "print(\"  3. Run predictions on test set\")\n",
    "print(\"  4. Compare results and create visualizations\")\n",
    "print(\"  5. Document findings\")\n",
    "\n",
    "print(\"\\nðŸ“Š Expected Outcomes:\")\n",
    "print(\"  - mHC should show more stable training (smoother loss)\")\n",
    "print(\"  - Both should be parameter-efficient (95%+ frozen)\")\n",
    "print(\"  - Comparison will reveal best approach for wound healing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

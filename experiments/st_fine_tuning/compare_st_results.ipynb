{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST Model Results Comparison\n",
    "\n",
    "Comprehensive comparison of all 5 ST model variants:\n",
    "\n",
    "| Model | LoRA | mHC | Velocity | Status |\n",
    "|-------|------|-----|----------|--------|\n",
    "| **ST-Tahoe (Baseline)** | ‚ùå | ‚ùå | ‚ùå | Pretrained |\n",
    "| **ST-LoRA** | ‚úÖ | ‚ùå | ‚ùå | Fine-tuned |\n",
    "| **ST-LoRA-mHC** | ‚úÖ | ‚úÖ | ‚ùå | Fine-tuned |\n",
    "| **ST-LoRA-Velocity** | ‚úÖ | ‚ùå | ‚úÖ | Fine-tuned |\n",
    "| **ST-LoRA-mHC-Velocity** | ‚úÖ | ‚úÖ | ‚úÖ | Fine-tuned |\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "1. **Training Dynamics**: Loss curves, gradient stability, convergence\n",
    "2. **Prediction Accuracy**: Gene correlation, cell-wise distance\n",
    "3. **Biological Validity**: Wound healing trajectories, cell-type responses\n",
    "4. **Efficiency**: Training time, memory usage, parameter count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model variants\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'st_tahoe_baseline',\n",
    "        'display_name': 'ST-Tahoe (Baseline)',\n",
    "        'prediction_file': 'experiments/st_fine_tuning/results/burn_sham_st_tahoe_predictions.h5ad',\n",
    "        'color': '#95A5A6',\n",
    "        'marker': 'o',\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora',\n",
    "        'display_name': 'ST-LoRA',\n",
    "        'prediction_file': '/home/scumpia-mrl/state_models/st_lora/predictions/burn_sham_predictions.h5ad',\n",
    "        'model_dir': '/home/scumpia-mrl/state_models/st_lora',\n",
    "        'color': '#3498DB',\n",
    "        'marker': 's',\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_mhc',\n",
    "        'display_name': 'ST-LoRA-mHC',\n",
    "        'prediction_file': '/home/scumpia-mrl/state_models/st_lora_mhc/predictions/burn_sham_predictions.h5ad',\n",
    "        'model_dir': '/home/scumpia-mrl/state_models/st_lora_mhc',\n",
    "        'color': '#E74C3C',\n",
    "        'marker': '^',\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_velocity',\n",
    "        'display_name': 'ST-LoRA-Velocity',\n",
    "        'prediction_file': '/home/scumpia-mrl/state_models/st_lora_velocity/predictions/burn_sham_predictions.h5ad',\n",
    "        'model_dir': '/home/scumpia-mrl/state_models/st_lora_velocity',\n",
    "        'color': '#2ECC71',\n",
    "        'marker': 'D',\n",
    "    },\n",
    "    {\n",
    "        'name': 'st_lora_mhc_velocity',\n",
    "        'display_name': 'ST-LoRA-mHC-Velocity',\n",
    "        'prediction_file': '/home/scumpia-mrl/state_models/st_lora_mhc_velocity/predictions/burn_sham_predictions.h5ad',\n",
    "        'model_dir': '/home/scumpia-mrl/state_models/st_lora_mhc_velocity',\n",
    "        'color': '#9B59B6',\n",
    "        'marker': '*',\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load predictions\n",
    "predictions = {}\n",
    "for config in model_configs:\n",
    "    if os.path.exists(config['prediction_file']):\n",
    "        predictions[config['name']] = ad.read_h5ad(config['prediction_file'])\n",
    "        print(f\"‚úÖ Loaded {config['display_name']}\")\n",
    "    else:\n",
    "        print(f\"‚è≥ {config['display_name']} - predictions not found\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(predictions)}/{len(model_configs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Dynamics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_logs(model_dir):\n",
    "    \"\"\"Load training metrics from CSV logs.\"\"\"\n",
    "    csv_files = glob.glob(f\"{model_dir}/lightning_logs/version_*/metrics.csv\")\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Load most recent version\n",
    "    csv_file = sorted(csv_files)[-1]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df\n",
    "\n",
    "# Load training logs\n",
    "training_logs = {}\n",
    "for config in model_configs:\n",
    "    if 'model_dir' in config:\n",
    "        logs = load_training_logs(config['model_dir'])\n",
    "        if logs is not None:\n",
    "            training_logs[config['name']] = logs\n",
    "            print(f\"‚úÖ Loaded logs for {config['display_name']}\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded logs for {len(training_logs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_logs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    for name, logs in training_logs.items():\n",
    "        config = next(c for c in model_configs if c['name'] == name)\n",
    "        \n",
    "        if 'train_loss' in logs.columns:\n",
    "            train_loss = logs['train_loss'].dropna()\n",
    "            axes[0].plot(\n",
    "                train_loss.index, \n",
    "                train_loss.values,\n",
    "                label=config['display_name'],\n",
    "                color=config['color'],\n",
    "                alpha=0.7,\n",
    "                linewidth=2\n",
    "            )\n",
    "    \n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    for name, logs in training_logs.items():\n",
    "        config = next(c for c in model_configs if c['name'] == name)\n",
    "        \n",
    "        if 'val_loss' in logs.columns:\n",
    "            val_loss = logs['val_loss'].dropna()\n",
    "            axes[1].plot(\n",
    "                val_loss.index,\n",
    "                val_loss.values,\n",
    "                label=config['display_name'],\n",
    "                color=config['color'],\n",
    "                alpha=0.7,\n",
    "                linewidth=2,\n",
    "                marker='o',\n",
    "                markersize=4\n",
    "            )\n",
    "    \n",
    "    axes[1].set_xlabel('Validation Step')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "    axes[1].set_title('Validation Loss Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiments/st_fine_tuning/results/loss_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Loss curves plotted\")\n",
    "else:\n",
    "    print(\"‚è≥ No training logs available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Stability (Loss Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_logs:\n",
    "    stability_metrics = []\n",
    "    \n",
    "    for name, logs in training_logs.items():\n",
    "        config = next(c for c in model_configs if c['name'] == name)\n",
    "        \n",
    "        if 'train_loss' in logs.columns:\n",
    "            train_loss = logs['train_loss'].dropna()\n",
    "            \n",
    "            # Compute stability metrics (last 20% of training)\n",
    "            cutoff = int(0.8 * len(train_loss))\n",
    "            final_losses = train_loss.iloc[cutoff:]\n",
    "            \n",
    "            stability_metrics.append({\n",
    "                'Model': config['display_name'],\n",
    "                'Final Loss (Mean)': final_losses.mean(),\n",
    "                'Loss Std': final_losses.std(),\n",
    "                'Loss Variance': final_losses.var(),\n",
    "                'Loss Range': final_losses.max() - final_losses.min(),\n",
    "            })\n",
    "    \n",
    "    stability_df = pd.DataFrame(stability_metrics)\n",
    "    stability_df = stability_df.sort_values('Loss Variance')\n",
    "    \n",
    "    print(\"\\nTraining Stability (Last 20% of Training):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(stability_df.to_string(index=False))\n",
    "    print(\"\\nüìä Lower variance = more stable training\")\n",
    "    print(\"   mHC should show lower variance than non-mHC variants\")\n",
    "else:\n",
    "    print(\"‚è≥ No training logs available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Quality Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "adata_true = ad.read_h5ad(\"experiments/baseline_analysis/data/burn_sham_baseline_embedded_2000.h5ad\")\n",
    "\n",
    "print(f\"Ground Truth Data:\")\n",
    "print(f\"  Shape: {adata_true.shape}\")\n",
    "print(f\"  Embeddings: {adata_true.obsm['X_state_2000'].shape}\")\n",
    "print(f\"  Conditions: {adata_true.obs['condition'].value_counts().to_dict()}\")\n",
    "\n",
    "# Separate burn and sham\n",
    "burn_true = adata_true[adata_true.obs['condition'] == 'Burn']\n",
    "sham_true = adata_true[adata_true.obs['condition'] == 'Sham']\n",
    "\n",
    "print(f\"\\n  Burn cells: {burn_true.shape[0]}\")\n",
    "print(f\"  Sham cells: {sham_true.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Embedding Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_correlation(pred_adata, true_adata, pred_key='X_state_2000', true_key='X_state_2000'):\n",
    "    \"\"\"Compute correlation between predicted and true embeddings.\"\"\"\n",
    "    \n",
    "    # Match cells by index\n",
    "    common_cells = pred_adata.obs_names.intersection(true_adata.obs_names)\n",
    "    \n",
    "    pred_emb = pred_adata[common_cells].obsm[pred_key]\n",
    "    true_emb = true_adata[common_cells].obsm[true_key]\n",
    "    \n",
    "    # Compute per-dimension correlation\n",
    "    dim_corrs = []\n",
    "    for i in range(pred_emb.shape[1]):\n",
    "        r, _ = pearsonr(pred_emb[:, i], true_emb[:, i])\n",
    "        dim_corrs.append(r)\n",
    "    \n",
    "    # Overall correlation (flattened)\n",
    "    overall_r, _ = pearsonr(pred_emb.flatten(), true_emb.flatten())\n",
    "    \n",
    "    # MSE and R2\n",
    "    mse = mean_squared_error(true_emb.flatten(), pred_emb.flatten())\n",
    "    r2 = r2_score(true_emb.flatten(), pred_emb.flatten())\n",
    "    \n",
    "    return {\n",
    "        'mean_correlation': np.mean(dim_corrs),\n",
    "        'median_correlation': np.median(dim_corrs),\n",
    "        'overall_correlation': overall_r,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'n_cells': len(common_cells),\n",
    "    }\n",
    "\n",
    "# Compute correlations for all models\n",
    "correlation_results = []\n",
    "\n",
    "for name, pred_adata in predictions.items():\n",
    "    config = next(c for c in model_configs if c['name'] == name)\n",
    "    \n",
    "    # Get burn predictions (comparing burn pred vs burn actual)\n",
    "    burn_pred = pred_adata[pred_adata.obs['condition'] == 'Burn']\n",
    "    \n",
    "    if len(burn_pred) > 0 and 'X_state_2000' in burn_pred.obsm:\n",
    "        metrics = compute_embedding_correlation(burn_pred, burn_true)\n",
    "        metrics['Model'] = config['display_name']\n",
    "        correlation_results.append(metrics)\n",
    "\n",
    "# Display results\n",
    "if correlation_results:\n",
    "    corr_df = pd.DataFrame(correlation_results)\n",
    "    corr_df = corr_df.sort_values('overall_correlation', ascending=False)\n",
    "    \n",
    "    print(\"\\nEmbedding Correlation (Burn Predictions vs Burn Ground Truth):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(corr_df[['Model', 'overall_correlation', 'mean_correlation', 'r2', 'mse']].to_string(index=False))\n",
    "    print(\"\\nüìä Higher correlation = better predictions\")\n",
    "else:\n",
    "    print(\"‚è≥ No predictions available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Perturbation Direction Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perturbation_direction(pred_adata, true_adata, pred_key='X_state_2000', true_key='X_state_2000'):\n",
    "    \"\"\"Compute consistency of perturbation direction (sham ‚Üí burn).\"\"\"\n",
    "    \n",
    "    # Match cells\n",
    "    burn_pred = pred_adata[pred_adata.obs['condition'] == 'Burn']\n",
    "    sham_pred = pred_adata[pred_adata.obs['condition'] == 'Sham']\n",
    "    \n",
    "    # Compute mean shift (sham ‚Üí burn)\n",
    "    pred_shift = burn_pred.obsm[pred_key].mean(axis=0) - sham_pred.obsm[pred_key].mean(axis=0)\n",
    "    true_shift = burn_true.obsm[true_key].mean(axis=0) - sham_true.obsm[true_key].mean(axis=0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cos_sim = np.dot(pred_shift, true_shift) / (np.linalg.norm(pred_shift) * np.linalg.norm(true_shift))\n",
    "    \n",
    "    # Magnitude ratio\n",
    "    pred_mag = np.linalg.norm(pred_shift)\n",
    "    true_mag = np.linalg.norm(true_shift)\n",
    "    mag_ratio = pred_mag / true_mag\n",
    "    \n",
    "    return {\n",
    "        'cosine_similarity': cos_sim,\n",
    "        'predicted_magnitude': pred_mag,\n",
    "        'true_magnitude': true_mag,\n",
    "        'magnitude_ratio': mag_ratio,\n",
    "    }\n",
    "\n",
    "# Compute perturbation directions\n",
    "direction_results = []\n",
    "\n",
    "for name, pred_adata in predictions.items():\n",
    "    config = next(c for c in model_configs if c['name'] == name)\n",
    "    \n",
    "    if 'X_state_2000' in pred_adata.obsm:\n",
    "        metrics = compute_perturbation_direction(pred_adata, adata_true)\n",
    "        metrics['Model'] = config['display_name']\n",
    "        direction_results.append(metrics)\n",
    "\n",
    "# Display results\n",
    "if direction_results:\n",
    "    dir_df = pd.DataFrame(direction_results)\n",
    "    dir_df = dir_df.sort_values('cosine_similarity', ascending=False)\n",
    "    \n",
    "    print(\"\\nPerturbation Direction Consistency (Sham ‚Üí Burn):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(dir_df.to_string(index=False))\n",
    "    print(\"\\nüìä Cosine similarity closer to 1.0 = better direction alignment\")\n",
    "    print(\"   Magnitude ratio closer to 1.0 = correct perturbation strength\")\n",
    "else:\n",
    "    print(\"‚è≥ No predictions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. UMAP Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions:\n",
    "    n_models = len(predictions)\n",
    "    fig, axes = plt.subplots(2, (n_models + 1) // 2, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (name, pred_adata) in enumerate(predictions.items()):\n",
    "        config = next(c for c in model_configs if c['name'] == name)\n",
    "        \n",
    "        # Compute UMAP\n",
    "        if 'X_state_2000' in pred_adata.obsm:\n",
    "            sc.pp.neighbors(pred_adata, use_rep='X_state_2000', n_neighbors=15)\n",
    "            sc.tl.umap(pred_adata)\n",
    "            \n",
    "            # Plot by condition\n",
    "            sc.pl.umap(\n",
    "                pred_adata,\n",
    "                color='condition',\n",
    "                ax=axes[i],\n",
    "                show=False,\n",
    "                title=config['display_name'],\n",
    "            )\n",
    "    \n",
    "    # Hide empty axes\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiments/st_fine_tuning/results/umap_comparison_all_models.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ UMAP comparison plotted\")\n",
    "else:\n",
    "    print(\"‚è≥ No predictions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ST MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Training Stability:\")\n",
    "if training_logs:\n",
    "    print(\"   Expected: mHC variants should have lower loss variance\")\n",
    "    if len(stability_df) > 0:\n",
    "        best_stable = stability_df.iloc[0]['Model']\n",
    "        print(f\"   ‚úÖ Most stable: {best_stable}\")\n",
    "else:\n",
    "    print(\"   ‚è≥ Training not completed yet\")\n",
    "\n",
    "print(\"\\n2. Prediction Accuracy:\")\n",
    "if correlation_results:\n",
    "    best_acc = corr_df.iloc[0]['Model']\n",
    "    best_r = corr_df.iloc[0]['overall_correlation']\n",
    "    print(f\"   ‚úÖ Best correlation: {best_acc} (r = {best_r:.3f})\")\n",
    "else:\n",
    "    print(\"   ‚è≥ Predictions not available yet\")\n",
    "\n",
    "print(\"\\n3. Perturbation Direction:\")\n",
    "if direction_results:\n",
    "    best_dir = dir_df.iloc[0]['Model']\n",
    "    best_cos = dir_df.iloc[0]['cosine_similarity']\n",
    "    print(f\"   ‚úÖ Best alignment: {best_dir} (cos = {best_cos:.3f})\")\n",
    "else:\n",
    "    print(\"   ‚è≥ Predictions not available yet\")\n",
    "\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - ST-LoRA: Good baseline, parameter-efficient\")\n",
    "print(\"   - ST-LoRA-mHC: Use if training instability observed\")\n",
    "print(\"   - ST-LoRA-Velocity: Use if velocity data available\")\n",
    "print(\"   - ST-LoRA-mHC-Velocity: Best overall (combines all benefits)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Comparison complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all metrics to CSV\n",
    "output_dir = Path(\"experiments/st_fine_tuning/results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if correlation_results:\n",
    "    corr_df.to_csv(output_dir / \"embedding_correlations.csv\", index=False)\n",
    "    print(\"‚úÖ Saved: embedding_correlations.csv\")\n",
    "\n",
    "if direction_results:\n",
    "    dir_df.to_csv(output_dir / \"perturbation_directions.csv\", index=False)\n",
    "    print(\"‚úÖ Saved: perturbation_directions.csv\")\n",
    "\n",
    "if training_logs:\n",
    "    stability_df.to_csv(output_dir / \"training_stability.csv\", index=False)\n",
    "    print(\"‚úÖ Saved: training_stability.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ All results exported to experiments/st_fine_tuning/results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3\n",
   "nbformat": 4,
   "nbformat_minor": 4
  }
 }
}
# LoRA + LoR2C-mHC Fine-Tuning Configuration for ST Model
# Fine-tune ST-Tahoe with LoRA adapters AND LoR2C-mHC for gradient stabilization
#
# LoR2C-mHC: Low-Rank Residual Connections with Manifold Hyper-Connections
# - Adds trainable low-rank residual paths alongside frozen transformer layers
# - Constrains residuals to doubly stochastic matrices (Birkhoff polytope)
# - Compatible with PEFT/LoRA (unlike layer-wrapping approaches)
#
# References:
# - LoR2C paper: https://arxiv.org/abs/2503.00572
# - mHC paper: https://arxiv.org/abs/2512.24880

model:
  name: "ST-LoRA-mHC"

  # LoRA Configuration
  lora:
    enable: true
    r: 16                        # LoRA rank
    alpha: 32                    # LoRA scaling factor (Î±/r = 2.0)
    dropout: 0.05                # LoRA dropout
    target_modules:              # Which modules to adapt
      - "c_attn"                 # Attention projections (Q, K, V)
      - "c_proj"                 # Output projections
    bias: "none"
    task_type: "FEATURE_EXTRACTION"

  # LoR2C-mHC Configuration (PEFT-compatible gradient stabilization)
  # NOTE: use_mhc is deprecated, use use_lor2c_mhc instead
  use_mhc: false
  use_lor2c_mhc: true
  lor2c_mhc:
    rank: 16                     # Low-rank dimension (r << hidden_dim)
    sinkhorn_iters: 20           # Sinkhorn-Knopp iterations for doubly stochastic projection
    alpha: 32.0                  # Scaling factor (like LoRA alpha)
    share_A: false               # If true, share A matrix across layers (ShareLoR2C variant)
    dropout: 0.0                 # Dropout on residual path

  # Model architecture
  input_dim: 2000                # State embedding dimension (X_state_2000)
  output_dim: 2000
  hidden_dim: 328
  cell_set_len: 512

  # Transformer backbone
  transformer_backbone_key: "GPT2"
  transformer_backbone_kwargs:
    n_embd: 328
    n_layer: 8
    n_head: 8
    n_positions: 512

  # Loss configuration
  loss: "energy"                 # Optimal transport loss (energy distance)
  blur: 0.05
  predict_residual: true

data:
  data_path: "../baseline_analysis/data/burn_sham_baseline_embedded_2000.h5ad"
  embed_key: "X_state_2000"
  pert_col: "condition"
  control_pert: "Sham"
  cell_type_key: "cell_types_simple_short"
  batch_col: "mouse_id"
  batch_size: 8
  num_workers: 0

training:
  max_steps: 5000
  learning_rate: 5e-5
  weight_decay: 0.0005
  warmup_steps: 100
  gradient_clip_val: 10
  val_freq: 500

  # Hardware
  devices: 1
  strategy: "auto"

output:
  output_dir: "/home/scumpia-mrl/state_models/st_lora_mhc"
  experiment_name: "st_lora_mhc_burn_sham"
  save_top_k: 2
  monitor: "val_loss"
  mode: "min"

# Notes:
# 1. LoR2C-mHC adds ~944K trainable params (for 8 layers, rank=16, hidden=328)
# 2. Combined with LoRA (~461K params), total trainable ~1.4M params
# 3. This is still very parameter-efficient vs full fine-tuning (~27M params)
# 4. Expected: Smoother loss curves, better gradient flow than LoRA alone

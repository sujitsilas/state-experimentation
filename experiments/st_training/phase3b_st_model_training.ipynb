{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3b: State Transition Model Training\n",
    "\n",
    "This notebook trains the Arc Institute State Transition model on burn/sham wound healing data.\n",
    "\n",
    "## Approach: Baseline SE-600M + Temporal Covariates\n",
    "\n",
    "We will:\n",
    "1. Validate data and configuration files\n",
    "2. Initialize the State Transition model with timepoint embeddings\n",
    "3. Train the model using PyTorch Lightning\n",
    "4. Monitor training progress\n",
    "5. Save best checkpoints\n",
    "\n",
    "**Key Decision**: Using baseline SE-600M embeddings (96% cell type accuracy) + temporal covariates instead of LoRA embeddings (75% cell type accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all required files exist\n",
    "required_files = {\n",
    "    'Data': '/home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/burn_sham_baseline_embedded.h5ad',\n",
    "    'TOML Config': 'examples/burn_sham.toml',\n",
    "    'YAML Config': 'configs/state_transition_burn_sham.yaml',\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREREQUISITE CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_exist = True\n",
    "for name, path in required_files.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n‚úì All required files exist!\")\n",
    "else:\n",
    "    print(\"\\n‚úó Some required files are missing. Please run phase3a first.\")\n",
    "    raise FileNotFoundError(\"Missing required files\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration\n",
    "config_path = \"configs/state_transition_burn_sham.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to verify it's ready\n",
    "data_path = \"/home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/burn_sham_baseline_embedded.h5ad\"\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "adata = ad.read_h5ad(data_path)\n",
    "\n",
    "print(f\"\\n‚úì Loaded AnnData:\")\n",
    "print(f\"  Shape: {adata.shape[0]} cells x {adata.shape[1]} genes\")\n",
    "print(f\"  Embeddings: {list(adata.obsm.keys())}\")\n",
    "print(f\"  Metadata: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Verify embeddings exist\n",
    "if 'X_state' not in adata.obsm:\n",
    "    raise ValueError(\"Baseline SE-600M embeddings (X_state) not found!\")\n",
    "\n",
    "print(f\"\\n‚úì Baseline embeddings shape: {adata.obsm['X_state'].shape}\")\n",
    "\n",
    "# Check data distribution\n",
    "print(f\"\\nData Distribution:\")\n",
    "print(f\"  Conditions: {adata.obs['condition'].value_counts().to_dict()}\")\n",
    "print(f\"  Timepoints: {adata.obs['timepoint'].value_counts().to_dict()}\")\n",
    "print(f\"  Cell types: {len(adata.obs['cell_types_simple_short'].unique())} unique\")\n",
    "print(f\"  Mice: {len(adata.obs['mouse_id'].unique())} unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct training command using Hydra-style overrides\n",
    "# The State CLI uses Hydra for configuration management\n",
    "\n",
    "output_dir = config['output']['output_dir']\n",
    "experiment_name = config['output']['experiment_name']\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_command = [\n",
    "    \"state\", \"tx\", \"train\",\n",
    "    \n",
    "    # Data configuration\n",
    "    f\"data.kwargs.toml_config_path={config['data']['toml_config_path']}\",\n",
    "    f\"data.kwargs.embed_key={config['data']['embed_key']}\",\n",
    "    f\"data.kwargs.pert_col={config['data']['pert_col']}\",\n",
    "    f\"data.kwargs.control_pert={config['data']['control_pert']}\",\n",
    "    f\"data.kwargs.cell_type_key={config['data']['cell_type_key']}\",\n",
    "    f\"data.kwargs.batch_col={config['data']['batch_col']}\",\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model=state\",  # Use state transition model\n",
    "    f\"model.kwargs.input_dim={config['model']['input_dim']}\",\n",
    "    f\"model.kwargs.output_dim={config['model']['output_dim']}\",\n",
    "    f\"model.kwargs.hidden_dim={config['model']['hidden_dim']}\",\n",
    "    f\"model.kwargs.cell_set_len={config['model']['cell_set_len']}\",\n",
    "    \n",
    "    # Timepoint embedding (our modification)\n",
    "    f\"model.kwargs.use_timepoint_embedding={config['model']['use_timepoint_embedding']}\",\n",
    "    f\"model.kwargs.num_timepoints=3\",  # day10, day14, day19\n",
    "    \n",
    "    # Training configuration\n",
    "    f\"training.max_steps={config['training']['max_steps']}\",\n",
    "    f\"training.batch_size={config['data']['batch_size']}\",\n",
    "    f\"training.learning_rate={config['training']['learning_rate']}\",\n",
    "    f\"training.devices={config['training']['devices']}\",\n",
    "    f\"training.strategy={config['training']['strategy']}\",\n",
    "    f\"training.gradient_clip_val={config['training']['gradient_clip_val']}\",\n",
    "    f\"training.val_check_interval={config['training']['val_check_interval']}\",\n",
    "    f\"training.log_every_n_steps={config['training']['log_every_n_steps']}\",\n",
    "    \n",
    "    # Output configuration\n",
    "    f\"output_dir={output_dir}\",\n",
    "    f\"name={experiment_name}\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING COMMAND\")\n",
    "print(\"=\" * 80)\n",
    "print(\" \\\\\\n  \".join(training_command))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate training time and resources\n",
    "num_cells = adata.shape[0]\n",
    "batch_size = config['data']['batch_size']\n",
    "max_steps = config['training']['max_steps']\n",
    "val_interval = config['training']['val_check_interval']\n",
    "num_gpus = config['training']['devices']\n",
    "\n",
    "steps_per_epoch = num_cells // (batch_size * num_gpus)\n",
    "num_epochs = max_steps / steps_per_epoch\n",
    "num_validations = max_steps // val_interval\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING ESTIMATES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Total cells: {num_cells:,}\")\n",
    "print(f\"  Batch size: {batch_size} per GPU\")\n",
    "print(f\"  Effective batch size: {batch_size * num_gpus} (across {num_gpus} GPUs)\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Max steps: {max_steps:,}\")\n",
    "print(f\"  Steps per epoch: ~{steps_per_epoch:,}\")\n",
    "print(f\"  Estimated epochs: ~{num_epochs:.1f}\")\n",
    "print(f\"  Validation frequency: every {val_interval} steps ({num_validations} times total)\")\n",
    "\n",
    "print(f\"\\nResources:\")\n",
    "print(f\"  GPUs: {num_gpus}x {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "print(f\"  Strategy: {config['training']['strategy'].upper()}\")\n",
    "\n",
    "print(f\"\\nExpected Duration:\")\n",
    "print(f\"  Training time: ~4-6 hours (estimated)\")\n",
    "print(f\"  Peak GPU memory: ~25-30 GB per GPU (estimated)\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Checkpoints: {output_dir}/{experiment_name}/checkpoints/\")\n",
    "print(f\"  Logs: {output_dir}/{experiment_name}/logs/\")\n",
    "print(f\"  TensorBoard: tensorboard --logdir={output_dir}/{experiment_name}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "\n",
    "‚ö†Ô∏è **Important**: Training will take 4-6 hours. Monitor progress with TensorBoard in a separate terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=/home/scumpia-mrl/state_models/st_burn_sham\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run training directly (blocking)\n",
    "# Uncomment to run training in this notebook (will block for 4-6 hours)\n",
    "\n",
    "# print(\"Starting training...\\n\")\n",
    "# result = subprocess.run(training_command, capture_output=False, text=True)\n",
    "# if result.returncode == 0:\n",
    "#     print(\"\\n‚úì Training completed successfully!\")\n",
    "# else:\n",
    "#     print(f\"\\n‚úó Training failed with return code {result.returncode}\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Training not started yet. Please choose one of the options below:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Run in Terminal (Recommended)\n",
    "\n",
    "Copy and paste this command into a terminal:\n",
    "\n",
    "```bash\n",
    "state tx train \\\n",
    "  data.kwargs.toml_config_path=examples/burn_sham.toml \\\n",
    "  data.kwargs.embed_key=X_state \\\n",
    "  data.kwargs.pert_col=condition \\\n",
    "  data.kwargs.control_pert=sham \\\n",
    "  data.kwargs.cell_type_key=cell_types_simple_short \\\n",
    "  data.kwargs.batch_col=mouse_id \\\n",
    "  model=state \\\n",
    "  model.kwargs.input_dim=2048 \\\n",
    "  model.kwargs.output_dim=2048 \\\n",
    "  model.kwargs.hidden_dim=512 \\\n",
    "  model.kwargs.cell_set_len=256 \\\n",
    "  model.kwargs.use_timepoint_embedding=true \\\n",
    "  model.kwargs.num_timepoints=3 \\\n",
    "  training.max_steps=20000 \\\n",
    "  training.batch_size=16 \\\n",
    "  training.learning_rate=0.0001 \\\n",
    "  training.devices=2 \\\n",
    "  training.strategy=ddp \\\n",
    "  training.gradient_clip_val=1.0 \\\n",
    "  training.val_check_interval=500 \\\n",
    "  training.log_every_n_steps=50 \\\n",
    "  output_dir=/home/scumpia-mrl/state_models/st_burn_sham \\\n",
    "  name=st_burn_sham_v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run in Background (tmux/screen)\n",
    "\n",
    "For long-running training, use tmux or screen:\n",
    "\n",
    "```bash\n",
    "# Start tmux session\n",
    "tmux new -s st_training\n",
    "\n",
    "# Run training command (same as above)\n",
    "state tx train ...\n",
    "\n",
    "# Detach: Ctrl+B, then D\n",
    "# Reattach: tmux attach -t st_training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training has started by looking for output directory\n",
    "training_dir = f\"{output_dir}/{experiment_name}\"\n",
    "\n",
    "if os.path.exists(training_dir):\n",
    "    print(f\"‚úì Training directory exists: {training_dir}\\n\")\n",
    "    \n",
    "    # Check for checkpoints\n",
    "    checkpoint_dir = f\"{training_dir}/checkpoints\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = sorted(Path(checkpoint_dir).glob(\"*.ckpt\"))\n",
    "        print(f\"Checkpoints found: {len(checkpoints)}\")\n",
    "        for ckpt in checkpoints[-5:]:  # Show last 5\n",
    "            size_mb = ckpt.stat().st_size / 1e6\n",
    "            print(f\"  - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"No checkpoints yet (training may just be starting)\")\n",
    "    \n",
    "    # Check for logs\n",
    "    print(\"\\nTo monitor training in real-time:\")\n",
    "    print(f\"  tensorboard --logdir={training_dir}\")\n",
    "else:\n",
    "    print(f\"‚úó Training has not started yet.\")\n",
    "    print(f\"  Expected directory: {training_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Training Logs (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse TensorBoard event files to extract training metrics\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "training_dir = f\"{output_dir}/{experiment_name}\"\n",
    "version_dir = Path(training_dir) / \"lightning_logs\" / \"version_0\"  # Adjust version as needed\n",
    "\n",
    "if version_dir.exists():\n",
    "    event_files = list(version_dir.glob(\"events.out.tfevents.*\"))\n",
    "    \n",
    "    if event_files:\n",
    "        print(f\"Loading metrics from: {version_dir}\\n\")\n",
    "        \n",
    "        ea = event_accumulator.EventAccumulator(str(version_dir))\n",
    "        ea.Reload()\n",
    "        \n",
    "        # Extract training loss\n",
    "        train_loss = ea.Scalars('train_loss')\n",
    "        val_loss = ea.Scalars('val_loss')\n",
    "        \n",
    "        # Convert to pandas for easier plotting\n",
    "        train_df = pd.DataFrame([(s.step, s.value) for s in train_loss], \n",
    "                               columns=['step', 'train_loss'])\n",
    "        val_df = pd.DataFrame([(s.step, s.value) for s in val_loss], \n",
    "                             columns=['step', 'val_loss'])\n",
    "        \n",
    "        # Plot training curves\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Training loss\n",
    "        axes[0].plot(train_df['step'], train_df['train_loss'], alpha=0.7)\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Training Loss')\n",
    "        axes[0].set_title('Training Loss Curve')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation loss\n",
    "        axes[1].plot(val_df['step'], val_df['val_loss'], color='orange', marker='o', markersize=3)\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Validation Loss')\n",
    "        axes[1].set_title('Validation Loss Curve')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/st_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úì Training curves saved to: figures/st_training_curves.png\")\n",
    "        \n",
    "        # Print final metrics\n",
    "        print(f\"\\nFinal Metrics:\")\n",
    "        print(f\"  Final training loss: {train_df['train_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Final validation loss: {val_df['val_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Best validation loss: {val_df['val_loss'].min():.4f}\")\n",
    "    else:\n",
    "        print(\"No TensorBoard event files found yet.\")\n",
    "else:\n",
    "    print(f\"Training logs directory not found: {version_dir}\")\n",
    "    print(\"Training may not have started or logs are in a different location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Identify Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the checkpoint with the lowest validation loss\n",
    "checkpoint_dir = f\"{output_dir}/{experiment_name}/checkpoints\"\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = list(Path(checkpoint_dir).glob(\"*.ckpt\"))\n",
    "    \n",
    "    if checkpoints:\n",
    "        print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "        \n",
    "        # Parse checkpoint filenames to extract validation loss\n",
    "        checkpoint_info = []\n",
    "        for ckpt in checkpoints:\n",
    "            name = ckpt.stem\n",
    "            size_mb = ckpt.stat().st_size / 1e6\n",
    "            \n",
    "            # Try to extract val_loss from filename (e.g., \"epoch=5-val_loss=0.1234.ckpt\")\n",
    "            if 'val_loss' in name:\n",
    "                val_loss = float(name.split('val_loss=')[1].split('-')[0])\n",
    "            else:\n",
    "                val_loss = float('inf')\n",
    "            \n",
    "            checkpoint_info.append({\n",
    "                'path': ckpt,\n",
    "                'name': ckpt.name,\n",
    "                'val_loss': val_loss,\n",
    "                'size_mb': size_mb\n",
    "            })\n",
    "        \n",
    "        # Sort by validation loss\n",
    "        checkpoint_info = sorted(checkpoint_info, key=lambda x: x['val_loss'])\n",
    "        \n",
    "        # Display top 5 checkpoints\n",
    "        print(\"Top 5 checkpoints (by validation loss):\\n\")\n",
    "        for i, info in enumerate(checkpoint_info[:5], 1):\n",
    "            print(f\"{i}. {info['name']}\")\n",
    "            print(f\"   Val Loss: {info['val_loss']:.4f}\")\n",
    "            print(f\"   Size: {info['size_mb']:.1f} MB\\n\")\n",
    "        \n",
    "        # Save best checkpoint path\n",
    "        best_checkpoint = checkpoint_info[0]['path']\n",
    "        print(f\"‚úì Best checkpoint: {best_checkpoint}\")\n",
    "        print(f\"\\nUse this checkpoint for evaluation in phase3c_st_evaluation.ipynb\")\n",
    "    else:\n",
    "        print(\"No checkpoints found yet.\")\n",
    "else:\n",
    "    print(f\"Checkpoint directory not found: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides the training setup for the State Transition model.\n",
    "\n",
    "### ‚úÖ Completed\n",
    "- Validated all prerequisites (data, configs)\n",
    "- Constructed training command with Hydra overrides\n",
    "- Provided options for running training (terminal, tmux/screen)\n",
    "- Set up monitoring tools (TensorBoard, checkpoint tracking)\n",
    "\n",
    "### üìã Training Details\n",
    "- **Model**: Arc Institute State Transition with timepoint embeddings\n",
    "- **Input**: Baseline SE-600M embeddings (2048-dim)\n",
    "- **Covariates**: Timepoint (day10/14/19), condition (burn/sham), cell type, mouse ID\n",
    "- **Training**: 20,000 steps, batch size 16, 2 GPUs (DDP)\n",
    "- **Duration**: ~4-6 hours\n",
    "\n",
    "### üéØ Modified Components\n",
    "- **State Transition Model** ([state_transition.py:200-210, 431-448](src/state/tx/models/state_transition.py)): Added timepoint encoder\n",
    "- **Dataset** ([scgpt_perturbation_dataset.py:175-212](src/state/tx/data/dataset/scgpt_perturbation_dataset.py)): Extracts timepoint_ids\n",
    "\n",
    "### ‚è≠Ô∏è Next Steps\n",
    "1. **Start training** using one of the methods above\n",
    "2. **Monitor progress** with TensorBoard\n",
    "3. **Wait for completion** (~4-6 hours)\n",
    "4. **Continue to phase3c_st_evaluation.ipynb** to evaluate predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

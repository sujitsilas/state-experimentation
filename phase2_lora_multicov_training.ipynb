{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: LoRA Multi-Covariate Fine-Tuning of SE-600M\n",
    "\n",
    "This notebook implements parameter-efficient fine-tuning of the STATE SE-600M embedding model using LoRA (Low-Rank Adaptation) adapters with multi-covariate conditioning.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Load Pretrained SE-600M**: Load the 600M parameter transformer model\n",
    "2. **Freeze Base Model**: Keep all pretrained weights frozen\n",
    "3. **Add LoRA Adapters**: Add low-rank trainable adapters to attention layers\n",
    "4. **Add Covariate Encoders**: Create embeddings for timepoint + condition\n",
    "5. **Condition Embeddings**: Combine base embeddings with covariate information\n",
    "6. **Fine-Tune**: Train only LoRA + covariate parameters (~1-5% of total params)\n",
    "\n",
    "## Key Differences from CPA Approach (Previous Incorrect Attempt)\n",
    "\n",
    "- ✅ **LoRA Fine-Tuning**: Works in embedding space, not perturbation prediction space\n",
    "- ✅ **SE-600M**: Modifies the foundation model itself, not a downstream task model\n",
    "- ✅ **Parameter Efficient**: Only trains ~1-5% of parameters vs. training entire CPA model\n",
    "- ✅ **Embedding Conditioning**: Covariates directly influence cell embeddings\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Base Model**: SE-600M (600M parameters, 16 transformer layers)\n",
    "- **LoRA Rank**: 16 (low-rank dimension)\n",
    "- **Covariates**: timepoint (3 categories) + condition (2 categories)\n",
    "- **Fusion**: Concatenation + MLP (512 → 256 → 2048)\n",
    "- **Training**: 2x RTX 5000 Ada, DDP, batch size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load burn/sham dataset\n",
    "data_path = \"/home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/burn_sham_data/burn_sham_processed.h5ad\"\n",
    "adata = ad.read_h5ad(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {adata.shape[0]} cells x {adata.shape[1]} genes\")\n",
    "print(f\"\\nObservations (metadata columns): {adata.obs.columns.tolist()}\")\n",
    "print(f\"\\nVariables (gene info): {adata.var.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate covariate columns\n",
    "required_cols = ['condition', 'timepoint', 'cell_types_simple_short', 'mouse_id']\n",
    "\n",
    "for col in required_cols:\n",
    "    if col in adata.obs.columns:\n",
    "        unique_vals = adata.obs[col].unique()\n",
    "        print(f\"✓ '{col}': {len(unique_vals)} unique values\")\n",
    "        print(f\"  Values: {unique_vals}\")\n",
    "        print(f\"  Distribution:\\n{adata.obs[col].value_counts()}\\n\")\n",
    "    else:\n",
    "        print(f\"✗ '{col}' NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Condition distribution\n",
    "adata.obs['condition'].value_counts().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Condition Distribution')\n",
    "axes[0].set_xlabel('Condition')\n",
    "axes[0].set_ylabel('Number of Cells')\n",
    "\n",
    "# Timepoint distribution\n",
    "adata.obs['timepoint'].value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Timepoint Distribution')\n",
    "axes[1].set_xlabel('Timepoint')\n",
    "axes[1].set_ylabel('Number of Cells')\n",
    "\n",
    "# Cell type distribution\n",
    "cell_type_counts = adata.obs['cell_types_simple_short'].value_counts().head(10)\n",
    "cell_type_counts.plot(kind='barh', ax=axes[2])\n",
    "axes[2].set_title('Top 10 Cell Types')\n",
    "axes[2].set_xlabel('Number of Cells')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA config\n",
    "config_path = \"configs/lora_multicov_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.state.emb.nn.lora_covariate_model import LoRACovariateStateModel\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading LoRA model...\")\n",
    "model = LoRACovariateStateModel(\n",
    "    base_checkpoint_path=config['base_checkpoint'],\n",
    "    covariate_config=config['covariates'],\n",
    "    lora_config=config['lora'],\n",
    "    learning_rate=config['training']['learning_rate'],\n",
    "    warmup_steps=config['training']['warmup_steps'],\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\nTrainable Parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LoRA Multi-Covariate Model Architecture\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. BASE MODEL (Frozen):\")\n",
    "print(f\"   - SE-600M Transformer: 16 layers, 16 heads, 2048 hidden dim\")\n",
    "print(f\"   - Token Encoder: Linear(5120 → 2048) + LayerNorm + SiLU\")\n",
    "print(f\"   - Transformer Encoder: 16x FlashTransformerEncoderLayer\")\n",
    "print(f\"   - Decoder: SkipBlock + Linear(2048 → 2048)\")\n",
    "print(f\"   - Status: ❄️ FROZEN (all 600M parameters)\")\n",
    "\n",
    "print(\"\\n2. LoRA ADAPTERS (Trainable):\")\n",
    "print(f\"   - Target: Attention Q, V projections\")\n",
    "print(f\"   - Rank: {config['lora']['r']}\")\n",
    "print(f\"   - Alpha: {config['lora']['lora_alpha']}\")\n",
    "print(f\"   - Dropout: {config['lora']['lora_dropout']}\")\n",
    "print(f\"   - Applied to: {len(config['lora']['target_modules'])} projection types × 16 layers\")\n",
    "\n",
    "print(\"\\n3. COVARIATE ENCODER (Trainable):\")\n",
    "for cov in config['covariates']['covariates']:\n",
    "    print(f\"   - {cov['name']}: {cov['type']} ({cov.get('num_categories', 'N/A')} categories) → {cov.get('embed_dim', 'N/A')} dim\")\n",
    "print(f\"   - Combination MLP: {config['covariates']['combination']['mlp_hidden_dims']} → {config['covariates']['combination']['mlp_output_dim']}\")\n",
    "\n",
    "print(\"\\n4. CONDITIONING PROJECTION (Trainable):\")\n",
    "print(f\"   - Input: Concat(base_embedding, covariate_embedding) = 4096 dim\")\n",
    "print(f\"   - Output: Conditioned embedding = 2048 dim\")\n",
    "print(f\"   - Architecture: Linear + LayerNorm + SiLU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Training is resource-intensive and should be run via the training script\n",
    "# This notebook demonstrates the setup and validation\n",
    "\n",
    "print(\"To start training, run:\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"python train_lora_multicov.py --config configs/lora_multicov_config.yaml\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExpected training time: 4-6 hours on 2x RTX 5000 Ada\")\n",
    "print(\"\\nMonitor training with TensorBoard:\")\n",
    "print(\"tensorboard --logdir=/home/scumpia-mrl/state_models/burn_sham_lora_multicov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Trained Model (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained checkpoint\n",
    "# checkpoint_path = \"/home/scumpia-mrl/state_models/burn_sham_lora_multicov/checkpoints/last.ckpt\"\n",
    "# trained_model = LoRACovariateStateModel.load_from_checkpoint(checkpoint_path)\n",
    "# trained_model.eval()\n",
    "# print(\"Trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract Covariate-Conditioned Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement embedding extraction\n",
    "# This will:\n",
    "# 1. Load trained model\n",
    "# 2. Process each cell with its covariates\n",
    "# 3. Extract conditioned embeddings\n",
    "# 4. Save to h5ad file\n",
    "\n",
    "print(\"Embedding extraction to be implemented after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs LoRA embeddings\n",
    "# - Cell type classification accuracy (kNN)\n",
    "# - Batch correction (silhouette scores)\n",
    "# - Temporal coherence\n",
    "# - UMAP visualization\n",
    "\n",
    "print(\"Evaluation to be implemented after training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook sets up LoRA-based multi-covariate fine-tuning of the SE-600M model.\n",
    "\n",
    "### Key Differences from Previous CPA Approach:\n",
    "\n",
    "| Aspect | CPA Approach (❌ Wrong) | LoRA Approach (✅ Correct) |\n",
    "|--------|------------------------|---------------------------|\n",
    "| **Model** | Trains downstream CPA task model | Fine-tunes SE-600M foundation model |\n",
    "| **Space** | Perturbation prediction space | Embedding space |\n",
    "| **Parameters** | Trains all CPA parameters (~20M) | Trains only LoRA + covariates (~1-5%) |\n",
    "| **Output** | Predicts perturbed gene expression | Produces covariate-conditioned embeddings |\n",
    "| **Use Case** | Specific to perturbation prediction | General-purpose embeddings for any downstream task |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Run training script: `python train_lora_multicov.py --config configs/lora_multicov_config.yaml`\n",
    "2. Monitor with TensorBoard\n",
    "3. Extract embeddings from trained model\n",
    "4. Evaluate and compare with baseline\n",
    "5. Use conditioned embeddings for downstream analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

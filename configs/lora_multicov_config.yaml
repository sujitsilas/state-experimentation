# LoRA Multi-Covariate Fine-Tuning Configuration
# For burn/sham wound healing dataset

# Base model paths
base_checkpoint: /home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/models/SE-600M/se600m_epoch16.ckpt
base_config: /home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/models/SE-600M/config.yaml

# Data configuration
data:
  h5ad_path: /home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/burn_sham_data/burn_sham_processed.h5ad
  batch_size: 16
  num_workers: 8
  # Column names in h5ad file
  covariate_columns:
    - condition      # burn vs sham
    - timepoint      # day10, day14, day19
  cell_type_col: cell_types_simple_short
  batch_col: mouse_id

# LoRA configuration
lora:
  r: 16                    # LoRA rank (low-rank dimension)
  lora_alpha: 32           # LoRA scaling factor (usually 2*r)
  lora_dropout: 0.1
  target_modules:
    - qkv_proj           # Combined Q, K, V projection in FlashTransformer
    - out_proj           # Output projection in attention
    # Optional: Add FFN projections
    # - linear1          # FFN first layer
    # - linear2          # FFN second layer
  bias: none

# Covariate encoder configuration
covariates:
  covariates:
    - name: condition
      type: categorical
      num_categories: 2    # burn, sham
      embed_dim: 256       # Larger than CPA to match SE-600M output (2048)

    - name: timepoint
      type: categorical
      num_categories: 3    # day10, day14, day19
      embed_dim: 256

    # Optional: Add continuous time encoding
    # - name: time_days
    #   type: continuous
    #   continuous_encoder:
    #     hidden_dim: 128
    #     output_dim: 256
    #     activation: silu

  combination:
    method: concat_mlp
    mlp_hidden_dims: [1024, 512]
    mlp_output_dim: 2048   # Match SE-600M output dimension
    dropout_rate: 0.1
    use_norm: layer

# Training configuration
training:
  max_epochs: 10
  learning_rate: 1e-4
  warmup_steps: 1000
  weight_decay: 0.01
  devices: 2              # Use 2 GPUs
  strategy: ddp           # Distributed Data Parallel
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  val_check_interval: 500

  # Logging
  log_every_n_steps: 50
  enable_progress_bar: true

  # Checkpointing
  save_top_k: 3
  monitor: val_loss
  mode: min

# Output configuration
output:
  output_dir: /home/scumpia-mrl/state_models/burn_sham_lora_multicov
  experiment_name: burn_sham_lora_multicov_v1
  save_embeddings: true
  embeddings_output: /home/scumpia-mrl/Desktop/Sujit/Projects/state-experimentation/burn_sham_lora_embedded.h5ad
